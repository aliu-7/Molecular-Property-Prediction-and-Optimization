In computational chemistry, many molecular datasets come in tabular form—where each molecule is described by a fixed-length vector of descriptors or fingerprints. These descriptors might include physicochemical properties (e.g., molecular weight, LogP), structural counts (e.g., number of aromatic rings or rotatable bonds), or binary fingerprints encoding substructure presence. When working with this kind of structured data, one of the most flexible and powerful machine learning tools available is the feedforward neural network (also called a fully connected neural network or multilayer perceptron).

Unlike RNNs that process SMILES strings or GNNs that operate on molecular graphs, feedforward neural networks treat molecular descriptors as static numerical vectors. This architecture makes them well-suited for classification and regression tasks in ADMET prediction, toxicity modeling, QSAR, and materials informatics.

Because neural networks are universal function approximators, they can learn highly nonlinear relationships between molecular structure and properties—sometimes outperforming simpler models like random forests or support vector machines when properly tuned. However, this flexibility comes at the cost of complexity. Designing, training, and interpreting neural networks requires care: too few neurons may limit performance, while too many may lead to overfitting.

In this chapter, you’ll learn: • How to prepare molecular descriptors for neural networks • How to design and train fully connected models in Python using TensorFlow/Keras • How to compare their performance to other algorithms such as random forests • And how to interpret the model’s predictions using modern explainability tools

Through a hands-on case study involving the BBBP dataset, you’ll build a complete pipeline from descriptor generation to model training and evaluation. By the end of the chapter, you’ll be able to confidently apply neural networks to a wide range of property prediction tasks using traditional descriptor-based representations.